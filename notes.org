* Methods
** Definitions
*** Covariance Matrix
    \begin{align}
    \left ( \mat{S}_\vec{x} \right ) _{i,j} =  cov \left ( \vec{x}_i - \vec{x}_j \right ) \\
    \end{align}
  Empirical covariance matrix for $n_s$ observations given as tensor $\mat{x} \in \rrr{11\times n_s}$:
    \begin{align}
    \mat{S} &= \frac{\left(\vec{x} - \bar{\vec{x}} \right) \left ( \vec{x} - \bar{\vec{x}} \right )^T}{n - 1}
    \end{align}
  Eigenvectors and -values of the covariance matrix can be computed directly from $\frac{\vec{x} - \bar{\vec{x}}}{\sqrt{n_s}}$ by
  computing the SVD:
  \begin{align}
  \frac{\vec{x} - \bar{\vec{x}}}{\sqrt{n_s}} &= \mat{U}\mat{\Lambda}\mat{V}
  \end{align}
  The columns of $\mat{U}$ then contain the eigenvectors of $\mat{S}$ and the diagonal entries of $\mat{\Lambda}$ the square roots
  of its eigenvalues

*** Singular Value Decomposition (SVD)
   \begin{align}
   \mat{M} = \mat{U} \mat{\Sigma} \mat{V}
   \end{align}

   + $\mat{M} \in \rrr{m \times n}$, $\mat{U} \in \rrr{m \times m}$ unitary, $\mat{\Sigma} \in \rrr{m \times n}$ rectangular diagonal, $\mat{V} \in \rrr{n \times n}$ unitary
   + singular values: diagonal entries of $\mat{Sigma}$
   + unitary matrix: $\mat{U}^*\mat{U} = \mat{I}$
   + left singular vectors of $\mat{M}$ are the eigenvectors of $\mat{M}\mat{M}^*$
   + right singular vectors of $\mat{M}$ are the eigenvectors of $\mat{M}^*\mat{M}$
   + use SVD to compute eigenvectors of sample covariance matrix

*** Correlation Coefficient Matrix
   Obtained from the /covariance matrix/ by normalizing by the corresponding variances
   \begin{align}
   \left ( \mat{S}_C \right )_{i,j} = \frac{\left ( \mat{S} \right ) _{i,j}}{ \sigma(x_i) \sigma(x_j)}
   \end{align}
    
*** Weighted Sums of Variables
   The variance of a weighted sum of random variables is given by the following formula:
   \begin{align}
   Var \left ( \sum_{i=1}^n a_i X_i \right ) = \sum_{i=1}^n a_i^2 Var(X_i) + 2 \sum_{i=1}\sum_{j=i+1}^n a_i a_j Cov(X_i, X_j)
   \end{align}
   For uncorrelated variables this simplifies to:
   \begin{align}
   Var \left ( \sum_{i=1}^n a_i X_i \right ) = \sum_{i=1}^n a_i^2 Var(X_i) 
   \end{align}
   
* ICI Simulation Database
** Noise Levels
    
    | Channel |  $\nu \:[\SI{}{\giga \hertz}]$ | $NE\Delta T$ |
    |---------+--------------------------------|--------------|
    |       1 |  183.31                        | 0.8          |
    |       2 |  183.31                        | 0.8          |
    |       3 |  183.31                        | 0.8          |
    |       4 | 243.2                          | 0.7          |
    |       5 | 325.15                         | 1.2          |
    |       6 | 325.15                         | 1.3          |
    |       7 | 325.15                         | 1.5          |
    |       8 | 448.0                          | 1.4          |
    |       9 | 448.0                          | 1.6          |
    |      10 | 448.0                          | 2.0          |
    |      11 | 664.2                          | 1.6          |

    - Computation of noise levels for PCAs:
      \begin{align}
      \vec{N}_{PCA}^2 &= \left(\mat{U}^2\right)^T \vec{N_{\DeltaT}}^2 \quad \text{(Element-wise Squares)}
      \end{align}

** Clear Sky
   - Detection threshold: $\sim \SI{10}{\gram \per \meter \squared}$
** Data Exploration
*** General
**** IWP
    [[./plots/general/iwp_dist.png]]
**** Surface Type
    [[./plots/general/surface_type_dist.png]]
**** Brightness Temperatures
    [[./plots/general/dtb_tb_dist.png]]
**** Optical Depth
    [[./plots/general/od_dist.png]]
**** PCA
    [[./plots/general/pca_channels.png]]
    [[./plots/general/pca_s.png]]
*** Clear Sky
**** IWP
    [[./plots/clear_sky/iwp_dist.png]]
**** Surface Type
[[./plots/clear_sky/st_dist.png]]
**** Brightness Temperatures
[[./plots/clear_sky/tb_dist.png]]
**** Optical Depth
[[./plots/clear_sky/od_dist.png]]
**** PCA
    [[./plots/clear_sky/pca_channels.png]]
    [[./plots/clear_sky/pca_s.png]]

*** Tropics (CWV > $\SI{40}{\kilo \gram \per \meter \squared}$)
**** CWV
    [[./plots/tropics/cwv_dist.png]]
**** IWP
    [[./plots/tropics/iwp_dist.png]]
**** Surface Type
   [[./plots/tropics/st_dist.png]]
**** Brightness Temperatures
   [[./plots/tropics/tb_dist.png]]
**** Optical Depth
    [[./plots/tropics/od_dist.png]]
**** PCA
    [[./plots/tropics/pca_channels.png]]
    [[./plots/tropics/pca_s.png]]

** Degrees of Freedom
*** DoFs Based on PCA Analysis
    - Number of degrees of freedom as PCA scores above corresponding noise level
    - Computed for different humidity and cloud cover regimes
      #+CAPTION: Degrees of Freedom w.r.t. CWV and IWP
    [[./plots/dofs/dofs.png]] 
      #+CAPTION: Degrees of Freedom w.r.t. CWV and IWP using only 8 components.
    [[./plots/dofs/dofs_pca_8.png]] 
      #+CAPTION: Degrees of Freedom w.r.t. CWV and IWP using only 6 components.
    [[./plots/dofs/dofs_pca_6.png]] 
      #+CAPTION: Degrees of Freedom w.r.t. CWV and IWP using only 4 components.
    [[./plots/dofs/dofs_pca_4.png]] 
      #+CAPTION: Number of simulations used in the computation of the degrees of freedom.
    [[./plots/dofs/counts.png]]
* Clear Sky Classification
** Data
   $m_{train} = 1794533$ ($80\%$) Mesaurements used for training, $m_{test} = 199393$ ($20\%$) used for testing. This is just for
   exploratory purposes and should be extended with at least on independent test set, that is used solely for
   evaluation purposes.
** Preprocessing
   Training data given as matrix $\mat{X} \in \rrr{m_{train} \times n}$ with $n = 11$. The input data is shifted so 
   that $\mathcal{E}(\mat{X}) = \mat{0}$

** Implementation
   For the training and evaluation of the networks the ~Keras~ python package with the ~theano~ backend is used.

** Training
   The training is performed over 2 epochs with a batch size of 32 applying /stochastic gradient descent/. The remaining settings are
   ~Keras~ default settings.

** Results
*** PCA and Noise
    How does a PCA affect the training of a neural network? What influence does the noise have. Different shallow and deep(er) networks are
    trained on the data. The shallow networks are MLPs with one hidden layer with $64$ neurons and /sigmoid/ activation and output functions. The
    deep(er) networks are similar in structure except that they contain two additional hidden layers with 64 neurons each and /sigmoid/ activation functions.

    #+CAPTION: Classifier performance on noisy training and test data.
    [[./plots/classification/clear sky/tpr_fpr_pca_shallow.png]]
    #+CAPTION: Classifier performance on noise-free training and test data.
    [[./plots/classification/clear sky/tpr_fpr_pca_shallow_no_noise.png]]
   
* MCI Trees
  The idea is to learn an importance sampling function that predicts the IWP from the ICI measurements.
** Algorithm Outline
   Each node $n$ in the tree represents a cluster of simulation results with IWP $x_n = \frac{\sum_i^N x_i}{N}$ and
   $\vec{y}_n = \frac{\sum_i^N \vec{y}_i}{N}$. Splitting is performed in a greedy fashion by splitting along the variable that
   yields the greatest decrease in the loss function. The loss function is chosen to be the squared error loss on the retrieved
   water vapor.
** General Formulation
*** Problem Statement
    We are given a set of observations consisting of a pair $(\mat{Y}, \vec{x})$ with $\mat{Y} \in \rrr{m \times n}$ and
    $\vec{x} \in \rrr{m}$ and $m$ the number of observations. We want to learn a vector of node values $\bar{\vec{x}} \in \rrr{n_T}$ and
    a matrix of node locations $\bar{\mat{Y}} \in \rrr{n_T \times n}$ such that for a given $\vec{y} \in \rrr{n}$
    \begin{align}
    \hat{x}(\vec{y}) &= \sum_j  \frac{ \bar{x}_j P(y | \bar{x}_j)}{\sum_k P(y | \bar{x}_k)}
    \end{align}
    approximates the true corresponding $x$. Here we assume for $p_j(y) = P(y | \bar{x}_j) = \exp \{ - (\vec{y} - \bar{y}_j) \mathbf{S}^{-1} (y - \bar{y}_j) \}$. 
    \end{align}
    
*** The Tree
    The node values and locations can be represented by a tree $\mathcal{T} = (\bar{\vec{x}}, \bar{\mat{Y}})$.
    
*** Loss Function
   Given a loss function $f: \rrr{} \mapsto \rrr{}$ a loss over a training set $(\mat{Y}, \vec{x})$ can be defined as:
     \begin{align}
     \mathcal{L}(\vec{x}, \mat{Y}, \bar{\vec{x}}, \bar{\mat{Y}}) &= \sum_i f \left(
    x_i - \sum_j  \frac{ \bar{x}_j p_j(y_i)}{\sum_k p_k(y_i)} \right )
     \end{align}
   The gradients w.r.t to the parameters describing the tree are given by:
     \begin{align}
     \frac{d \mathcal{L}(\vec{x}, \mat{Y}, \vec{\bar{x}}, \bar{\mat{Y}})}{d\bar{x_j}}
 &= - \sum_i \frac{p_j(\vec{y}_i)}{P(\vec{y}_i)} \frac{df}{dx}(x_i, \vec{Y}_i, \bar{\vec{x}}, \bar{\mat{Y}}) \\
     \frac{d \mathcal{L}(\vec{x}, \mat{Y}, \vec{\bar{x}}, \bar{\mat{Y}})}{d\bar{\vec{y}_j}}
 &= - \sum_i \left (
            \frac{\frac{d p_j(\vec{y}_i)}{d\vec{\bar{y}}_j}}{P(\vec{y}_i)} 
            - \frac{\frac{d p_j(\vec{y}_i)}{d\vec{\bar{y}}_j}}{P(\vec{y}_i)^2} \right ) \frac{df}{dx}(x_i, \vec{Y}_i, \bar{\vec{x}}, \bar{\mat{Y}}) 
     \end{align}

*** Squared Error Loss
    For squared error loss the gradients thus take the form:

     \begin{align}
     \frac{d \mathcal{L}(\vec{x}, \mat{Y}, \vec{\bar{x}}, \bar{\mat{Y}})}{d\bar{x_j}}
 &= - 2 \sum_i \frac{p_j(\vec{y}_i)}{P(\vec{y}_i)} \left ( x_i - \sum_j \frac{\bar{x}_j p_j(\vec{y}_i)}{\sum_k p_k(\vec{y}_i)} \right ) \\
     \frac{d \mathcal{L}(\vec{x}, \mat{Y}, \vec{\bar{x}}, \bar{\mat{Y}})}{d\bar{\vec{y}_j}}
 &= - \sum_i \left (
            \frac{\frac{d p_j(\vec{y}_i)}{d\vec{\bar{y}}_j}}{P(\vec{y}_i)} 
            - \frac{\frac{d p_j(\vec{y}_i)}{d\vec{\bar{y}}_j}}{P(\vec{y}_i)^2} \right ) \left ( x_i - \sum_j \frac{\bar{x}_j p_j(\vec{y}_i)}{\sum_k p_k(\vec{y}_i)} \right )
     \end{align}
   
    The derivatives $\frac{d p_j(\vec{y})}{d\bar{\vec{y}_j}}$ are given by:
    \begin{align}
\frac{d p_j(\vec{y})}{d\bar{\vec{y}_j}} &=
 2 \mat{S}^{-1}(\vec{y} - \bar{\vec{y}_j}) 
\exp \left \{ - (\vec{y} - \bar{\vec{y}_j}) \mathbf{S}^{-1} (\vec{y} - \bar{\vec{y}}_j) \right \}
    \end{align}

* PCA & BMCI
  Dimensionality reduction may help BMCI method to obtain better coverage in measurement-state space. A first approach would
  by to use only 8 PCA components to compress the data.
* Paper Notes
** A Hotelling Transformation Approach for Rapid Inversion of Atmospheric Data
   Investigates data reduction techniques based on eigenvector expansions (/hotelling transformations/).
*** Inversion Techniques
    OEM and neural networks as inversion techniques are presented with focus on the need (or advantages)
    of dimensionality reduction of the measurement vector $\vec{y}$.
   
*** Hotelling Transformations
    The most common hotelling transformation is PCA (Kahunen-Loeve transformation, EOF) and is based on an
    eigenvector expansion of the (empirical) measurement covariance matrix $\mat{S}_\vec{y}$:
      \begin{align}
      \mathbf{S}_\mathbf{y} = \mathbf{E}\mathbf{\Lambda}\mathbf{E}^T
      \end{align}
   Another possibility is to use an eigenvector expansion of the corresponding /correlation coefficient matrix/.
   
   The newly presented transformation in this paper is based on using an OEM approach to compute $\mat{S}_\vec{y}$:
      \begin{align}
      \mat{S}_\vec{y} &= \mat{K}_\vec{x}^T \mat{S}_\vec{x} \mat{K}_\vec{x} + \mat{K}_\vec{b}^T \mat{S}_\vec{b} \mat{K}_\vec{b} + \mat{S}_\epsilon
      \end{align}
      Only /physical/ information is considered, i.e. the second and third term are ignored. This has similarities with the
      truncated SVD retrieval method.

*** Information Content
    To compare the reduction techniques, the information content of the retrieval w.r.t to $k$ (the number of eigenvalues used
    in the expansion)  and the measurement error for $k=50$ are used for a number of different simulated retrievals.
    \begin{align}
    H &= \frac{1}{2} \log_2 |\mat{S}_\delta^{-1} | \\
    \Delta H &= \frac{1}{2} \log_2 |\mat{S}_\delta^{-1} | - \frac{1}{2} \log_2 |\mat{S}_\vec{x}^{-1} | \\
    \end{align}
    where $|\mat{S}_\delta^{-1}|$: determinant of the inverse of the measurement error
***   Results
    The newly proposed methods performs comparable to the method based on $\mat{S}_\vec{y}$ for noise-free data. Otherwise
    it performs even better. Also provides the possibility of improving the reduction for specific species.
** Structured Probabilistic Models
   
